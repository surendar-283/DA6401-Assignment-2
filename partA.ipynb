{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "19hWdLBjRlPatxpEOVxDE6ObXmsu0ENpw",
      "authorship_tag": "ABX9TyOXHjgwVnxawfu6F2m9vo9i"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import functools"
      ],
      "metadata": {
        "id": "GRfU_DUK6vIY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_activation_function(activation_name):\n",
        "  \"\"\"Create and return the specified activation function.\"\"\"\n",
        "  activation_mapping = {\n",
        "      'relu': nn.ReLU(),\n",
        "      'leaky_relu': nn.LeakyReLU(0.1),\n",
        "      'gelu': nn.GELU()\n",
        "  }\n",
        "  return activation_mapping.get(activation_name, nn.ReLU())"
      ],
      "metadata": {
        "id": "VO8DsWP-6t9w"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_convolutional_block(in_channels, out_channels, kernel_size,use_batchnorm, dropout_rate, activation_fn):\n",
        "    \"\"\"Construct a convolutional block with optional batchnorm and dropout.\"\"\"\n",
        "    layers = [\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, padding='same'),\n",
        "        activation_fn\n",
        "    ]\n",
        "\n",
        "    if use_batchnorm:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "\n",
        "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "    if dropout_rate > 0:\n",
        "        layers.append(nn.Dropout(dropout_rate))\n",
        "\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "W3LtFSTa6sRg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_flattened_size(conv_blocks, input_shape):\n",
        "    \"\"\"Calculate the flattened size after convolutional layers.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        dummy = torch.zeros(1, input_shape[2], input_shape[0], input_shape[1])\n",
        "        for block in conv_blocks:\n",
        "            dummy = block(dummy)\n",
        "        return dummy.view(1, -1).shape[1]"
      ],
      "metadata": {
        "id": "vFdONnVi6qoI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dense_layers(flattened_size, dense_units, dropout_rate, activation_fn, num_classes):\n",
        "    \"\"\"Construct dense layers with optional dropout.\"\"\"\n",
        "    layers = [nn.Flatten()]\n",
        "    prev_size = flattened_size\n",
        "\n",
        "    for units in dense_units:\n",
        "        layers.extend([\n",
        "            nn.Linear(prev_size, units),\n",
        "            activation_fn\n",
        "        ])\n",
        "        if dropout_rate > 0:\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "        prev_size = units\n",
        "\n",
        "    layers.append(nn.Linear(prev_size, num_classes))\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "CuN8rIGg6pLx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create forward function\n",
        "def forward_pass(x, conv_blocks, dense_block):\n",
        "    for block in conv_blocks:\n",
        "        x = block(x)\n",
        "    return dense_block(x)"
      ],
      "metadata": {
        "id": "8I2nAZVp7KEg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vision_network(config, input_shape=(224, 224, 3), num_classes=10):\n",
        "    \"\"\"Create a vision neural network based on configuration parameters.\"\"\"\n",
        "    # Extract configuration parameters\n",
        "    conv_filters = config.get('conv_filters', [32, 32, 32, 32, 32])\n",
        "    kernel_sizes = config.get('kernel_sizes', [3, 3, 3, 3, 3])\n",
        "    dense_units = config.get('dense_units', [128])\n",
        "    dropout_rate = config.get('dropout_rate', 0.2)\n",
        "    use_batchnorm = config.get('use_batchnorm', True)\n",
        "    activation_name = config.get('activation', 'relu')\n",
        "\n",
        "    # Create activation function\n",
        "    activation_fn = create_activation_function(activation_name)\n",
        "\n",
        "    # Build convolutional blocks\n",
        "    conv_blocks = []\n",
        "    in_channels = input_shape[2]\n",
        "\n",
        "    for i in range(len(conv_filters)):\n",
        "        apply_dropout = (i < len(conv_filters) - 1) and (dropout_rate > 0)\n",
        "        block = build_convolutional_block(\n",
        "            in_channels,\n",
        "            conv_filters[i],\n",
        "            kernel_sizes[i],\n",
        "            use_batchnorm,\n",
        "            dropout_rate if apply_dropout else 0,\n",
        "            activation_fn\n",
        "        )\n",
        "        conv_blocks.append(block)\n",
        "        in_channels = conv_filters[i]\n",
        "\n",
        "    # Calculate flattened size\n",
        "    flattened_size = calculate_flattened_size(conv_blocks, input_shape)\n",
        "\n",
        "    # Build dense layers\n",
        "    dense_block = build_dense_layers(\n",
        "        flattened_size,\n",
        "        dense_units,\n",
        "        dropout_rate,\n",
        "        activation_fn,\n",
        "        num_classes\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Create a complete model\n",
        "    model = nn.Module()\n",
        "    model.conv_blocks = nn.ModuleList(conv_blocks)\n",
        "    model.dense_block = dense_block\n",
        "    model.forward = functools.partial(forward_pass, conv_blocks=model.conv_blocks, dense_block=model.dense_block)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "xqgUea-N6nkB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_transforms(augment=False):\n",
        "    \"\"\"Prepare image transformations for training and validation.\"\"\"\n",
        "    base_transforms = [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]\n",
        "\n",
        "    if augment:\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            *base_transforms\n",
        "        ])\n",
        "    else:\n",
        "        train_transforms = transforms.Compose(base_transforms)\n",
        "\n",
        "    test_transforms = transforms.Compose(base_transforms)\n",
        "\n",
        "    return train_transforms, test_transforms"
      ],
      "metadata": {
        "id": "0iHVP_Dv6cyC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_split_dataset(data_dir, train_transform, test_transform, val_split=0.2):\n",
        "    \"\"\"Load dataset and split into train, validation, and test sets.\"\"\"\n",
        "    # Load datasets\n",
        "    train_dataset = ImageFolder(os.path.join(data_dir, 'train'), transform=train_transform)\n",
        "    test_dataset = ImageFolder(os.path.join(data_dir, 'val'), transform=test_transform)\n",
        "\n",
        "    # Split train into train and validation\n",
        "    val_size = int(val_split * len(train_dataset))\n",
        "    train_size = len(train_dataset) - val_size\n",
        "    train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    return train_subset, val_subset, test_dataset, train_dataset.classes"
      ],
      "metadata": {
        "id": "cJp-7zTf6bcB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_loaders(train_data, val_data, test_data, batch_size=32):\n",
        "    \"\"\"Create data loaders for training, validation, and testing.\"\"\"\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "Uf7xblfu6Z27"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(data_dir, batch_size=32, val_split=0.2, augment=False):\n",
        "    \"\"\"Prepare data loaders for training, validation, and testing.\"\"\"\n",
        "    # Prepare transforms\n",
        "    train_transform, test_transform = prepare_transforms(augment)\n",
        "\n",
        "    # Load and split dataset\n",
        "    train_data, val_data, test_data, classes = load_and_split_dataset(\n",
        "        data_dir, train_transform, test_transform, val_split\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader, val_loader, test_loader = create_data_loaders(\n",
        "        train_data, val_data, test_data, batch_size\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader, classes"
      ],
      "metadata": {
        "id": "0KxywtGv6YKE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, loss_fn, device):\n",
        "    \"\"\"Evaluate model on the given data loader.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            # Calculate metrics\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    # Calculate average loss and accuracy\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = 100.0 * correct / total\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "YHrbW1Ek6WnS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_loader, optimizer, loss_fn, device):\n",
        "    \"\"\"Train model for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with tqdm(train_loader, unit=\"batch\") as progress_bar:\n",
        "        for inputs, labels in progress_bar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate metrics\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{100.0 * correct / total:.1f}%'\n",
        "            })\n",
        "\n",
        "    # Calculate average loss and accuracy\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100.0 * correct / total\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "bkwiB0n96S5j"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate(config=None):\n",
        "    \"\"\"Train and validate model with the given configuration.\"\"\"\n",
        "    with wandb.init(config=config) as run:\n",
        "        config = wandb.config\n",
        "\n",
        "        # Set descriptive run name\n",
        "        run.name = (f\"filters_{'-'.join(map(str, config.conv_filters))}_\"\n",
        "                   f\"dense_{'-'.join(map(str, config.dense_units))}_\"\n",
        "                   f\"lr_{config.learning_rate:.0e}_\"\n",
        "                   f\"bs_{config.batch_size}\")\n",
        "\n",
        "        # Prepare data\n",
        "        train_loader, val_loader, _, classes = prepare_data(\n",
        "            data_dir='/content/drive/MyDrive/inaturalist_12K',\n",
        "            batch_size=config.batch_size,\n",
        "            augment=config.data_augmentation\n",
        "        )\n",
        "\n",
        "        # Create model\n",
        "        model = create_vision_network(\n",
        "            {\n",
        "                'conv_filters': config.conv_filters,\n",
        "                'kernel_sizes': config.kernel_sizes,\n",
        "                'dense_units': config.dense_units,\n",
        "                'dropout_rate': config.dropout_rate,\n",
        "                'use_batchnorm': config.use_batchnorm,\n",
        "                'activation': config.activation\n",
        "            },\n",
        "            num_classes=len(classes)\n",
        "        )\n",
        "\n",
        "        # Set device\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = model.to(device)\n",
        "\n",
        "        # Initialize optimizer and loss function\n",
        "        optimizer = optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr=config.learning_rate,\n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(1, 21):  # Fixed 20 epochs\n",
        "            # Train for one epoch\n",
        "            train_loss, train_acc = train_epoch(\n",
        "                model, train_loader, optimizer, loss_fn, device\n",
        "            )\n",
        "\n",
        "            # Validate\n",
        "            val_loss, val_acc = evaluate_model(\n",
        "                model, val_loader, loss_fn, device\n",
        "            )\n",
        "\n",
        "            # Log metrics\n",
        "            wandb.log({\n",
        "                'epoch': epoch,\n",
        "                'train_loss': train_loss,\n",
        "                'train_acc': train_acc,\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc\n",
        "            })"
      ],
      "metadata": {
        "id": "vGEl9agt6Nny"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_sweep_config():\n",
        "    \"\"\"Set up configuration for hyperparameter sweep.\"\"\"\n",
        "    return {\n",
        "        'method': 'bayes',\n",
        "        'metric': {'name': 'val_acc', 'goal': 'maximize'},\n",
        "        'parameters': {\n",
        "            'conv_filters': {\n",
        "                'values': [\n",
        "                    [32, 32, 32, 32, 32],\n",
        "                    [64, 64, 64, 64, 64],\n",
        "                    [16, 32, 64, 128, 256],\n",
        "                    [256, 128, 64, 32, 16]\n",
        "                ]\n",
        "            },\n",
        "            'kernel_sizes': {\n",
        "                'values': [\n",
        "                    [3, 3, 3, 3, 3],\n",
        "                    [5, 5, 5, 5, 5],\n",
        "                    [3, 5, 3, 5, 3]\n",
        "                ]\n",
        "            },\n",
        "            'dense_units': {\n",
        "                'values': [\n",
        "                    [64],\n",
        "                    [128],\n",
        "                    [64, 128],\n",
        "                    [256, 128]\n",
        "                ]\n",
        "            },\n",
        "            'learning_rate': {\n",
        "                'values': [1e-3, 1e-4]\n",
        "            },\n",
        "            'weight_decay': {\n",
        "                'values': [0, 0.0001, 0.001, 0.01]\n",
        "            },\n",
        "            'dropout_rate': {\n",
        "                'values': [0.0, 0.2, 0.3, 0.5]\n",
        "            },\n",
        "            'use_batchnorm': {\n",
        "                'values': [True, False]\n",
        "            },\n",
        "            'batch_size': {\n",
        "                'values': [32, 64, 128]\n",
        "            },\n",
        "            'data_augmentation': {\n",
        "                'values': [True, False]\n",
        "            },\n",
        "            'activation': {\n",
        "                'values': ['relu', 'leaky_relu', 'gelu']\n",
        "            }\n",
        "        }\n",
        "    }"
      ],
      "metadata": {
        "id": "kQM6aNw56JxT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main function to run the hyperparameter sweep.\"\"\"\n",
        "    # Initialize wandb\n",
        "    wandb.login(key=\"49f8f505158ee3693f0cacf0a82118bd4e636e8c\")\n",
        "\n",
        "    # Set up sweep configuration\n",
        "    sweep_config = setup_sweep_config()\n",
        "\n",
        "    # Create and run sweep\n",
        "    sweep_id = wandb.sweep(sweep_config, project='DA6401_A2')\n",
        "    wandb.agent(sweep_id, function=train_and_validate, count=30)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "D-vRSxLy6BXk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}